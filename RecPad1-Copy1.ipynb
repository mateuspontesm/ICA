{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports e Definindo os Classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.covariance import empirical_covariance\n",
    "from numpy.linalg import pinv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Classificador NN com distância euclidiana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MateusLindoNN():\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        labels = []\n",
    "        for vec in X_test:\n",
    "            dist = [(np.linalg.norm(vec - self.X_train[ii]), l)\n",
    "                    for ii, l in enumerate(self.y_train)]  # d:distance, l: label\n",
    "            nearestL = min(dist, key=lambda x: x[0])[1]\n",
    "            labels.append(nearestL)\n",
    "        return labels\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        return accuracy_score(y_test, self.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Classificador NN com distância de Mahanalobis modificada:\n",
    "\n",
    "Na literatura e em implementações como a Scikit-learn, o KNN com distância de Mahanalobis é feito utilizando a matriz de covariância de todo o conjunto de treinamento, e isso faz sentido para o caso do KNN com K>1, mas para o caso do K=1 é possível implementar com uma matriz de covariância distinta para cada classe. Na comparação dos classificadores implementados com os da biblioteca Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MateusLindoNNMaha():\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.classes = np.unique(y_train)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        dataClasses = {}\n",
    "        self.means = {}\n",
    "        self.covs = {}\n",
    "        self.ranks = {}\n",
    "        self.conds = {}\n",
    "        self.invs = {}\n",
    "        self.cov = np.cov(X_train, rowvar= False)\n",
    "        self.inv = pinv(self.cov)\n",
    "        for clas in self.classes:\n",
    "            dataClasses[clas] = X_train[y_train == clas]\n",
    "            self.means[clas] = np.mean(dataClasses[clas],axis=0)  # mean of every feature for class 'clas' \n",
    "            self.covs[clas] = np.cov(dataClasses[clas],rowvar=False)  #covariance matrix of class\n",
    "            self.ranks[clas] = np.linalg.matrix_rank(dataClasses[clas]) # rank of the data matrix for each class\n",
    "            self.conds[clas] = np.linalg.cond(dataClasses[clas])  # condition number of matrix\n",
    "            self.invs[clas] = pinv(self.covs[clas]) #inverse of covariance\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        labels = []\n",
    "        for vec in X_test:\n",
    "            dist = []\n",
    "            #dist = [( (vec - self.X_train[ii])@self.invs[l]@(vec - self.X_train[ii]).T , l) for ii, l in enumerate(self.y_train)]  # d:distance, l: label \n",
    "            dist = [( (vec - self.X_train[ii])@self.invs[l]@(vec - self.X_train[ii]).T , l) for ii, l in enumerate(self.y_train)]  # d:distance, l: label \n",
    "            nearestL = min(dist, key=lambda x: x[0])[1]\n",
    "            labels.append(nearestL)\n",
    "        return labels\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        return accuracy_score(y_test, self.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Distância Mínima ao Centróide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MateusLindoNC():\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        centers = []\n",
    "        classes = {c: [] for c in np.unique(y_train)}\n",
    "        for (x, y) in zip(X_train, y_train):\n",
    "            classes[y].append(x)\n",
    "        for k in classes:\n",
    "            center = np.mean(classes[k], axis=0)\n",
    "            centers.append((center,k))\n",
    "        self.centers = centers\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        labels = []\n",
    "        for vec in X_test:\n",
    "            distLabel = [(np.linalg.norm(vec - center[0]), center[1]) for center in self.centers]  # tuple: (distance, label)\n",
    "            nearest = min(distLabel, key=lambda x: x[0])[1]\n",
    "            labels.append(nearest)\n",
    "        return labels\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        return accuracy_score(y_test, self.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Classificador Quadrático Gaussiano:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem regularização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MateusLindoCQG():\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.classes = np.unique(y_train)\n",
    "        dataClasses = {}\n",
    "        self.means = {}\n",
    "        self.covs = {}\n",
    "        self.ranks = {}\n",
    "        self.conds = {}\n",
    "        self.invs = {}\n",
    "        for clas in self.classes:\n",
    "            dataClasses[clas] = X_train[y_train == clas]\n",
    "            self.means[clas] = np.mean(dataClasses[clas],axis=0)  # mean of every feature for class 'clas' \n",
    "            self.covs[clas] = np.cov(dataClasses[clas], rowvar=False)  #covariance matrix of class\n",
    "            self.ranks[clas] = np.linalg.matrix_rank(dataClasses[clas]) # rank of the data matrix for each class\n",
    "            self.conds[clas] = np.linalg.cond(dataClasses[clas])  # condition number of matrix\n",
    "            self.invs[clas] = np.linalg.inv(self.covs[clas]) #inverse of covariance\n",
    "        \n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        labels = []\n",
    "        for vec in X_test:\n",
    "            #  tuple (pred_value, class) \n",
    "            preds = [ ((vec - self.means[c])@self.invs[c]@(vec - self.means[c]).T + np.log(np.linalg.det(self.covs[c])), c) for c in self.classes]\n",
    "            label = min(preds, key = lambda x: x[0])[1]\n",
    "            labels.append(label)\n",
    "        return labels\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        return accuracy_score(y_test, self.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com Regularização de Friedman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MateusLindoCQGF():\n",
    "    \n",
    "    def __init__(self, alpha = 0.5):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.classes = np.unique(y_train)\n",
    "        dataClasses = {}\n",
    "        self.means = {}\n",
    "        self.covs = {}\n",
    "        self.ranks = {}\n",
    "        self.conds = {}\n",
    "        self.invs = {}\n",
    "        need_reg = False\n",
    "\n",
    "        for clas in self.classes:\n",
    "            dataClasses[clas] = X_train[y_train == clas]\n",
    "            self.means[clas] = np.mean(dataClasses[clas],axis=0)  # mean of every feature for class 'clas' \n",
    "            self.covs[clas] = np.cov(dataClasses[clas], rowvar=False)  #covariance matrix of class\n",
    "            self.ranks[clas] = np.linalg.matrix_rank(self.covs[clas]) # rank of the data matrix for each class\n",
    "            self.conds[clas] = np.linalg.cond(self.covs[clas])  # condition number of matrix\n",
    "            if self.ranks[clas] == len(self.covs[clas]): # Caso seja de rank completo\n",
    "                self.invs[clas] = np.linalg.inv(self.covs[clas]) #inverse of covariance\n",
    "            else: need_reg = True\n",
    "        \n",
    "        if need_reg:\n",
    "            covX = np.cov(X_train,rowvar=False)\n",
    "            for clas in self.classes:\n",
    "                self.covs[clas] = (1-self.alpha) * self.covs[clas] + (self.alpha)*covX\n",
    "                self.invs[clas] = np.linalg.inv(self.covs[clas])    \n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        labels = []\n",
    "        for vec in X_test:\n",
    "            #  tuple (pred_value, class) \n",
    "            # reg\n",
    "            preds = [ ((vec - self.means[c])@self.invs[c]@(vec - self.means[c]).T + np.log(np.linalg.det(self.covs[c])), c) for c in self.classes] #com regularização\n",
    "            label = min(preds, key = lambda x: x[0])[1]\n",
    "            labels.append(label)\n",
    "        return labels\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        return accuracy_score(y_test, self.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Importando os dados: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('parkinsons.data',header=0,index_col=0)\n",
    "dfY = df['status']\n",
    "dfX = df.drop('status',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Avaliando sem escalar os dados\n",
    "\n",
    "Neste passo avaliamos o desempenho dos classificadores implementados e comparamos o desempenho deles com os mesmos classificadores, mas com a implementação da biblioteca Scikit-Learn. Os objetivos deste passo são:\n",
    "1.  Avaliar se a nossa implementação está correta ao comparar com implementações mais sólidas.\n",
    "2.  Obter os mesmos resultados nos classificadores Vizinho mais Próximo, Classificador Quadrático Gaussiano e Centróide mais Próximo.\n",
    "3.  Comparar o desempenho da nossa implementação diferenciada do Vizinho mais Próximo com distância de Mahalanobis com a implementação padrão do Scikit-Learn.\n",
    "\n",
    "Para verificar os resultados acima serão feitas 1000 rodadas do experimento, cada rodada vai nos dar um valor de acurácia para cada classificador, esta acurácia será armazenada em um vetor para cada classificador que será utilizado mais a frente.\n",
    "Podemos instanciar os classificadores antes, com exceção do NN com distância de Mahalanobis do SciKit-Learn, pois ele necessita de um parametro *V* que é a matriz de covariância."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Instanciando os Classificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNclass = KNeighborsClassifier(n_neighbors=1)\n",
    "Ncenter = NearestCentroid()\n",
    "QClass = QuadraticDiscriminantAnalysis()\n",
    "QClassF = QuadraticDiscriminantAnalysis(reg_param=0.5)\n",
    "mateusNN = MateusLindoNN()\n",
    "mateusNC = MateusLindoNC()\n",
    "mateusCQG =  MateusLindoCQG()\n",
    "mateusCQGF = MateusLindoCQGF()\n",
    "mateusMaha = MateusLindoNNMaha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8205128205128205 0.6923076923076923 0.8717948717948718 0.7435897435897436 0.8974358974358975\n",
      "0.8205128205128205 0.6923076923076923 0.8717948717948718 0.7948717948717948 0.8461538461538461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateuspontes/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfX,dfY,test_size = 0.2)\n",
    "\n",
    "QClass.fit(X_train,y_train)\n",
    "Ncenter.fit(X_train,y_train)\n",
    "NNclass.fit(X_train,y_train)\n",
    "NNMaha.fit(X_train, y_train)\n",
    "QClassF.fit(X_train, y_train)\n",
    "mateusNN.fit(X_train.values, y_train.values)\n",
    "mateusNC.fit(X_train.values, y_train.values)\n",
    "mateusCQG.fit(X_train.values, y_train.values)\n",
    "mateusCQGF.fit(X_train.values, y_train.values)\n",
    "mateusMaha.fit(X_train.values, y_train.values)\n",
    "\n",
    "print(NNclass.score(X_test,y_test),Ncenter.score(X_test,y_test), QClass.score(X_test,y_test),QClassF.score(X_test,y_test), NNMaha.score(X_test,y_test))\n",
    "print(mateusNN.score(X_test.values,y_test.values), mateusNC.score(X_test.values,y_test.values), mateusCQG.score(X_test.values,y_test.values),mateusCQGF.score(X_test.values,y_test.values), mateusMaha.score(X_test.values,y_test.values) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Simulação de Monte Carlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateuspontes/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "skNN =[]\n",
    "skNC = []\n",
    "skCQG = []\n",
    "skCQGF = []\n",
    "skNNMaha = []\n",
    "acMateusNN = []\n",
    "acMateusNC = []\n",
    "acMateusCQG = []\n",
    "acMateusCQGF = []\n",
    "acMateusNNMaha = []\n",
    "\n",
    "for ii in range(1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfX,dfY,test_size = 0.2)\n",
    "    NNMaha = KNeighborsClassifier(n_neighbors=1, metric='mahalanobis', metric_params={'V': np.cov(X_train,rowvar=False)})\n",
    "    ## SKLEARN ##########\n",
    "    QClass.fit(X_train,y_train)\n",
    "    QClassF.fit(X_train,y_train)\n",
    "    Ncenter.fit(X_train,y_train)\n",
    "    NNclass.fit(X_train,y_train)\n",
    "    NNMaha.fit(X_train, y_train)\n",
    "    ## HANDCODED #########\n",
    "    mateusNN.fit(X_train.values, y_train.values)\n",
    "    mateusNC.fit(X_train.values, y_train.values)\n",
    "    mateusCQG.fit(X_train.values, y_train.values)\n",
    "    mateusCQGF.fit(X_train.values, y_train.values)\n",
    "    mateusMaha.fit(X_train.values, y_train.values)\n",
    "    ### Acuracy Vectors ######\n",
    "    skNN.append(NNclass.score(X_test,y_test))\n",
    "    skNC.append(Ncenter.score(X_test,y_test))\n",
    "    skCQG.append(QClass.score(X_test,y_test))\n",
    "    skCQGF.append(QClassF.score(X_test,y_test))\n",
    "    skNNMaha.append(NNMaha.score(X_test,y_test))\n",
    "    acMateusNN.append(mateusNN.score(X_test.values,y_test.values))\n",
    "    acMateusNC.append(mateusNC.score(X_test.values,y_test.values)) \n",
    "    acMateusCQG.append(mateusCQG.score(X_test.values,y_test.values))\n",
    "    acMateusCQGF.append(mateusCQGF.score(X_test.values,y_test.values))\n",
    "    acMateusNNMaha.append(mateusMaha.score(X_test.values,y_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Avaliação dos classificadores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN e DMC\n",
    "Os classificadores NN e DMC implementados possuem os mesmos valores de acurácia dos algoritmos da biblioteca Scikit-Learn, como podemos ver abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(skNN == acMateusNN)\n",
    "print(skNC == acMateusNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CQG sem Regularização\n",
    "* Já o nosso CQG possui desempenho similar na média, 0.8790512820512822 do Scikit-Learn contra 0.8787435897435899 da nossa implementação. \n",
    "* Além disso os valores variam no máximo de 8% entre as implementações. \n",
    "   \n",
    "É possível concluir que a diferença entre as implementações está relacionada ao cálculo da matriz de covariância, apesar de saber do problema do nosso algoritmo não foi possível alcançar um desempenho idêntico mesmo utilizando a Regularização de Friedman e a matriz de covariância Pooled. Para efeito de demonstração faremos outra simulação de Monte Carlo mais a frente comparado estes algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8786666666666668 0.8766923076923079\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(skCQG),np.mean(acMateusCQG))\n",
    "print(np.allclose(skCQG,acMateusCQG,atol=0.08))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CQG com Regularização\n",
    "\n",
    "O CQG com regularização de Friedman possui desempenho inferior ao CQG sem regularização, com a diferença entre as médias chegando a 1,5%. Apesar desse adendo é importante frisar que a regularização efetuada pelo Scikit-Learn é diferente da de Friedman, ja que ela utiliza a equação abaixo:\n",
    "\n",
    "$$ (1- reg\\_param)\\Sigma + (reg\\_param) \\, I_{n\\_features} $$\n",
    "Onde $\\Sigma$ é a matriz de covariância. Já a nossa implementação utiliza a regularização dada pela equação abaixo:\n",
    "$$ \\Sigma_k = (1- \\alpha) \\Sigma_k + \\alpha \\Sigma $$\n",
    "Onde $\\Sigma_k$ é a matriz de covariância da classe $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7651282051282052 0.7500256410256411\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(skCQGF),np.mean(acMateusCQGF))\n",
    "print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN com Distância de Mahalanobis\n",
    "\n",
    "Já para a comparação do nosso algoritmo do NN com distância de Mahalanobis foi possível verificar que existe a nossa implementação é 1% superior na média em relação ao algoritmo de NN com distância de Mahalanobis padrão do Scikit-Learn. Um ponto a ser considerado é a diferença no cálculo da inversa da matriz de covariância da biblioteca do Scikit-Learn, como mostrado na comparação dos CQGs, um ponto a ser estudado posteriormente é se utilizando o cálculo semelhante ao do Scikit-Learn é possível melhorar ainda mais o desempenho desta versão diferenciada do algoritmo.\n",
    "\n",
    "* Comparando as médias obtemos os valores de 0.8702820512820515 (Scikit-Learn) e 0.8801025641025643 (Implementação diferenciada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8695897435897437 0.857769230769231\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(skNNMaha),np.mean(acMateusNNMaha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação dos algoritmos:\n",
    "\n",
    "Para fazermos uma comparação mais minuciosa dos classificadores podemos construir uma tabela com os valores da média, mediana, mínimo e máximo, desvio padrão, sensibilidade e especifidade. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Procurando o Melhor Escalador para cada Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9743589743589743 0.7435897435897436 0.8717948717948718\n"
     ]
    }
   ],
   "source": [
    "#Classificadores\n",
    "NNclass = KNeighborsClassifier(n_neighbors=1)\n",
    "Ncenter = NearestCentroid()\n",
    "QClass = LinearDiscriminantAnalysis()\n",
    "#Pipelines:\n",
    "stepsNN = [('scaler', StandardScaler()),\n",
    "         ('classifier', NNclass)]\n",
    "stepsNC = [('scaler', StandardScaler()),\n",
    "         ('classifier', Ncenter)]\n",
    "stepsQC = [('scaler', StandardScaler()),\n",
    "         ('classifier', QClass)]\n",
    "pipelineNN = Pipeline(stepsNN)\n",
    "pipelineNC = Pipeline(stepsNC)\n",
    "pipelineQC = Pipeline(stepsQC)\n",
    "#Fits e Scores\n",
    "pipelineNN.fit(X_train,y_train)\n",
    "pipelineNC.fit(X_train,y_train)\n",
    "pipelineQC.fit(X_train,y_train)\n",
    "print(pipelineNN.score(X_test,y_test), pipelineNC.score(X_test,y_test), pipelineQC.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9743589743589743 0.7948717948717948 0.8974358974358975\n"
     ]
    }
   ],
   "source": [
    "#Classificadores\n",
    "NNclass = KNeighborsClassifier(n_neighbors=1)\n",
    "Ncenter = NearestCentroid()\n",
    "QClass = LinearDiscriminantAnalysis()\n",
    "#Pipelines:\n",
    "stepsNN = [('scaler', StandardScaler()),\n",
    "         ('classifier', NNclass)]\n",
    "stepsNC = [('scaler', MinMaxScaler()),\n",
    "         ('classifier', Ncenter)]\n",
    "stepsQC = [('scaler', Normalizer()),\n",
    "         ('classifier', QClass)]\n",
    "pipelineNN = Pipeline(stepsNN)\n",
    "pipelineNC = Pipeline(stepsNC)\n",
    "pipelineQC = Pipeline(stepsQC)\n",
    "#Fits e Scores\n",
    "pipelineNN.fit(X_train,y_train)\n",
    "pipelineNC.fit(X_train,y_train)\n",
    "pipelineQC.fit(X_train,y_train)\n",
    "print(pipelineNN.score(X_test,y_test), pipelineNC.score(X_test,y_test), pipelineQC.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
